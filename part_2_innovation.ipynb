{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\bonda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle as pkl\n",
    "import networkx as nx \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Doc2Vec Model\n",
    "from gensim.models.doc2vec import Doc2Vec as d2v\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sentiment Analysis\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Topic Model\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bonda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bonda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define Tokenization\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "# Define word root process\n",
    "def get_lemma(word):\n",
    "    # get root\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "# Define Stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "en_stop = list(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# preprocessing function utilizing tokenization, extracting root words, without removing stop words\n",
    "def prepare_text_for_d2v(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- column for number of child node sibilings\n",
    "- column for similarity measure between child node and root node \n",
    "- use combo of hours and days for time dif columns\n",
    "- column for count of comments made between parent and child\n",
    "- identify root as question or comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make cyclical nodes have depth 0 and scale up everything else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bonda\\AppData\\Local\\Temp/ipykernel_10500/2825944186.py:111: RuntimeWarning: invalid value encountered in true_divide\n",
      "  norm_distances = (root_distances - root_distances.mean()) / root_distances.std()\n",
      "C:\\Users\\bonda\\AppData\\Local\\Temp/ipykernel_10500/2825944186.py:112: RuntimeWarning: invalid value encountered in true_divide\n",
      "  norm_root_distances = (distances - distances.mean()) / distances.std()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph</th>\n",
       "      <th>node</th>\n",
       "      <th>node_text</th>\n",
       "      <th>tokenized_node_text</th>\n",
       "      <th>parent</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>tokenized_parent_text</th>\n",
       "      <th>node_children</th>\n",
       "      <th>parent_children</th>\n",
       "      <th>num_node_sibilings</th>\n",
       "      <th>depth</th>\n",
       "      <th>votes</th>\n",
       "      <th>root_time_dif</th>\n",
       "      <th>parent_time_dif</th>\n",
       "      <th>comments_from_parent_to_child</th>\n",
       "      <th>edited</th>\n",
       "      <th>root_is_question</th>\n",
       "      <th>polarity</th>\n",
       "      <th>distance</th>\n",
       "      <th>root_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.136</td>\n",
       "      <td>Many of the Western World countries have divor...</td>\n",
       "      <td>[many, the, western, world, country, have, div...</td>\n",
       "      <td>10035.43</td>\n",
       "      <td>'[Optimism bias](https://www.sciencedirect.com...</td>\n",
       "      <td>[optimism, URL, science, article, pii, s096098...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10035.136]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10 days 23:33:22</td>\n",
       "      <td>9 days 01:30:30</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.681160</td>\n",
       "      <td>-1.037725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.137</td>\n",
       "      <td>This argument does not take into account wheth...</td>\n",
       "      <td>[this, argument, doe, not, take, into, account...</td>\n",
       "      <td>10035.83</td>\n",
       "      <td>The fact that women are more likely to achieve...</td>\n",
       "      <td>[the, fact, that, woman, are, more, likely, ac...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10035.137, 10035.84, 10035.149]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10 days 23:36:24</td>\n",
       "      <td>4 days 18:12:36</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.125298</td>\n",
       "      <td>-0.423591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.133</td>\n",
       "      <td>Life is a constantly changing experience, and ...</td>\n",
       "      <td>[life, constantly, change, experience, and, it...</td>\n",
       "      <td>10035.93</td>\n",
       "      <td>Many people make fundamental life-altering cha...</td>\n",
       "      <td>[many, people, make, fundamental, life, alteri...</td>\n",
       "      <td>[10035.148]</td>\n",
       "      <td>[10035.133, 10035.122]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10 days 22:50:06</td>\n",
       "      <td>3 days 20:52:35</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.039752</td>\n",
       "      <td>0.901820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.232</td>\n",
       "      <td>Marriage should be treated like a business par...</td>\n",
       "      <td>[marriage, should, treat, like, business, part...</td>\n",
       "      <td>10035.107</td>\n",
       "      <td>As marriage loses ground in the modern world, ...</td>\n",
       "      <td>[marriage, lose, ground, the, modern, world, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10035.232, 10035.110, 10035.168, 10035.170, 1...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35 days 10:20:58</td>\n",
       "      <td>25 days 10:50:46</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.017162</td>\n",
       "      <td>0.286632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.8</td>\n",
       "      <td>Prenuptial agreements can protect individuals ...</td>\n",
       "      <td>[prenuptial, agreement, can, protect, individu...</td>\n",
       "      <td>10035.1</td>\n",
       "      <td>All couples should sign a prenuptial agreement...</td>\n",
       "      <td>[all, couple, should, sign, prenuptial, agreem...</td>\n",
       "      <td>[10035.38, 10035.14, 10035.11, 10035.146, 1003...</td>\n",
       "      <td>[10035.8, 10035.4, 10035.5, 10035.6, 10035.7, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days 01:16:41</td>\n",
       "      <td>0 days 01:16:41</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.624799</td>\n",
       "      <td>-0.793625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323580</th>\n",
       "      <td>9997</td>\n",
       "      <td>9997.151</td>\n",
       "      <td>It will increase support for the far-right and...</td>\n",
       "      <td>[will, increase, support, for, the, far, right...</td>\n",
       "      <td>9997.33</td>\n",
       "      <td>Political backlash will be incited.</td>\n",
       "      <td>[political, backlash, will, incite]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[9997.49, 9997.34, 9997.151]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>548 days 08:55:14</td>\n",
       "      <td>520 days 14:57:48</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.793904</td>\n",
       "      <td>-1.023541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323581</th>\n",
       "      <td>9997</td>\n",
       "      <td>9997.157</td>\n",
       "      <td>Workers will have to file a civil claim agains...</td>\n",
       "      <td>[worker, will, have, file, civil, claim, again...</td>\n",
       "      <td>9997.155</td>\n",
       "      <td>It will be difficult for these migrant workers...</td>\n",
       "      <td>[will, difficult, for, these, migrant, worker,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[9997.159, 9997.157]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>549 days 12:23:12</td>\n",
       "      <td>0 days 00:03:53</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.393857</td>\n",
       "      <td>-0.082730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323582</th>\n",
       "      <td>9997</td>\n",
       "      <td>9997.135</td>\n",
       "      <td>Higher immigration levels were beneficial at a...</td>\n",
       "      <td>[higher, immigration, level, be, beneficial, t...</td>\n",
       "      <td>9997.55</td>\n",
       "      <td>The US may have a beneficial increase in popul...</td>\n",
       "      <td>[the, may, have, beneficial, increase, populat...</td>\n",
       "      <td>[9997.139, 9997.137]</td>\n",
       "      <td>[9997.169, 9997.135]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>548 days 08:11:45</td>\n",
       "      <td>447 days 15:15:48</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.020503</td>\n",
       "      <td>-0.344498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323583</th>\n",
       "      <td>9997</td>\n",
       "      <td>9997.139</td>\n",
       "      <td>According to the Pew Research Center, of the n...</td>\n",
       "      <td>[accord, the, pew, research, center, the, near...</td>\n",
       "      <td>9997.135</td>\n",
       "      <td>Higher immigration levels were beneficial at a...</td>\n",
       "      <td>[higher, immigration, level, be, beneficial, t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[9997.139, 9997.137]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>548 days 08:22:49</td>\n",
       "      <td>0 days 00:11:04</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.417212</td>\n",
       "      <td>0.607781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323584</th>\n",
       "      <td>9997</td>\n",
       "      <td>9997.137</td>\n",
       "      <td>According to a June 2018 Rasmussen poll, [51%]...</td>\n",
       "      <td>[accord, june, 2018, rasmussen, poll, URL, pub...</td>\n",
       "      <td>9997.135</td>\n",
       "      <td>Higher immigration levels were beneficial at a...</td>\n",
       "      <td>[higher, immigration, level, be, beneficial, t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[9997.139, 9997.137]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>548 days 08:19:19</td>\n",
       "      <td>0 days 00:07:34</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.863611</td>\n",
       "      <td>-0.372984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323585 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        graph       node                                          node_text  \\\n",
       "0       10035  10035.136  Many of the Western World countries have divor...   \n",
       "1       10035  10035.137  This argument does not take into account wheth...   \n",
       "2       10035  10035.133  Life is a constantly changing experience, and ...   \n",
       "3       10035  10035.232  Marriage should be treated like a business par...   \n",
       "4       10035    10035.8  Prenuptial agreements can protect individuals ...   \n",
       "...       ...        ...                                                ...   \n",
       "323580   9997   9997.151  It will increase support for the far-right and...   \n",
       "323581   9997   9997.157  Workers will have to file a civil claim agains...   \n",
       "323582   9997   9997.135  Higher immigration levels were beneficial at a...   \n",
       "323583   9997   9997.139  According to the Pew Research Center, of the n...   \n",
       "323584   9997   9997.137  According to a June 2018 Rasmussen poll, [51%]...   \n",
       "\n",
       "                                      tokenized_node_text     parent  \\\n",
       "0       [many, the, western, world, country, have, div...   10035.43   \n",
       "1       [this, argument, doe, not, take, into, account...   10035.83   \n",
       "2       [life, constantly, change, experience, and, it...   10035.93   \n",
       "3       [marriage, should, treat, like, business, part...  10035.107   \n",
       "4       [prenuptial, agreement, can, protect, individu...    10035.1   \n",
       "...                                                   ...        ...   \n",
       "323580  [will, increase, support, for, the, far, right...    9997.33   \n",
       "323581  [worker, will, have, file, civil, claim, again...   9997.155   \n",
       "323582  [higher, immigration, level, be, beneficial, t...    9997.55   \n",
       "323583  [accord, the, pew, research, center, the, near...   9997.135   \n",
       "323584  [accord, june, 2018, rasmussen, poll, URL, pub...   9997.135   \n",
       "\n",
       "                                              parent_text  \\\n",
       "0       '[Optimism bias](https://www.sciencedirect.com...   \n",
       "1       The fact that women are more likely to achieve...   \n",
       "2       Many people make fundamental life-altering cha...   \n",
       "3       As marriage loses ground in the modern world, ...   \n",
       "4       All couples should sign a prenuptial agreement...   \n",
       "...                                                   ...   \n",
       "323580                Political backlash will be incited.   \n",
       "323581  It will be difficult for these migrant workers...   \n",
       "323582  The US may have a beneficial increase in popul...   \n",
       "323583  Higher immigration levels were beneficial at a...   \n",
       "323584  Higher immigration levels were beneficial at a...   \n",
       "\n",
       "                                    tokenized_parent_text  \\\n",
       "0       [optimism, URL, science, article, pii, s096098...   \n",
       "1       [the, fact, that, woman, are, more, likely, ac...   \n",
       "2       [many, people, make, fundamental, life, alteri...   \n",
       "3       [marriage, lose, ground, the, modern, world, w...   \n",
       "4       [all, couple, should, sign, prenuptial, agreem...   \n",
       "...                                                   ...   \n",
       "323580                [political, backlash, will, incite]   \n",
       "323581  [will, difficult, for, these, migrant, worker,...   \n",
       "323582  [the, may, have, beneficial, increase, populat...   \n",
       "323583  [higher, immigration, level, be, beneficial, t...   \n",
       "323584  [higher, immigration, level, be, beneficial, t...   \n",
       "\n",
       "                                            node_children  \\\n",
       "0                                                      []   \n",
       "1                                                      []   \n",
       "2                                             [10035.148]   \n",
       "3                                                      []   \n",
       "4       [10035.38, 10035.14, 10035.11, 10035.146, 1003...   \n",
       "...                                                   ...   \n",
       "323580                                                 []   \n",
       "323581                                                 []   \n",
       "323582                               [9997.139, 9997.137]   \n",
       "323583                                                 []   \n",
       "323584                                                 []   \n",
       "\n",
       "                                          parent_children num_node_sibilings  \\\n",
       "0                                             [10035.136]                  0   \n",
       "1                        [10035.137, 10035.84, 10035.149]                  2   \n",
       "2                                  [10035.133, 10035.122]                  1   \n",
       "3       [10035.232, 10035.110, 10035.168, 10035.170, 1...                  6   \n",
       "4       [10035.8, 10035.4, 10035.5, 10035.6, 10035.7, ...                 14   \n",
       "...                                                   ...                ...   \n",
       "323580                       [9997.49, 9997.34, 9997.151]                  2   \n",
       "323581                               [9997.159, 9997.157]                  1   \n",
       "323582                               [9997.169, 9997.135]                  1   \n",
       "323583                               [9997.139, 9997.137]                  1   \n",
       "323584                               [9997.139, 9997.137]                  1   \n",
       "\n",
       "       depth votes     root_time_dif   parent_time_dif  \\\n",
       "0          4     0  10 days 23:33:22   9 days 01:30:30   \n",
       "1          5     0  10 days 23:36:24   4 days 18:12:36   \n",
       "2          2     0  10 days 22:50:06   3 days 20:52:35   \n",
       "3          2     0  35 days 10:20:58  25 days 10:50:46   \n",
       "4          1     0   0 days 01:16:41   0 days 01:16:41   \n",
       "...      ...   ...               ...               ...   \n",
       "323580     2     0 548 days 08:55:14 520 days 14:57:48   \n",
       "323581     5     0 549 days 12:23:12   0 days 00:03:53   \n",
       "323582     2     0 548 days 08:11:45 447 days 15:15:48   \n",
       "323583     3     0 548 days 08:22:49   0 days 00:11:04   \n",
       "323584     3     0 548 days 08:19:19   0 days 00:07:34   \n",
       "\n",
       "       comments_from_parent_to_child edited root_is_question polarity  \\\n",
       "0                                 93      1                0        1   \n",
       "1                                 54      0                0       -1   \n",
       "2                                 40      0                0       -1   \n",
       "3                                125      1                0        1   \n",
       "4                                  7      0                0        1   \n",
       "...                              ...    ...              ...      ...   \n",
       "323580                           118      0                1        1   \n",
       "323581                             2      0                1        1   \n",
       "323582                            80      1                1       -1   \n",
       "323583                             4      0                1        1   \n",
       "323584                             2      0                1        1   \n",
       "\n",
       "        distance  root_distance  \n",
       "0      -1.681160      -1.037725  \n",
       "1       0.125298      -0.423591  \n",
       "2       0.039752       0.901820  \n",
       "3      -1.017162       0.286632  \n",
       "4      -1.624799      -0.793625  \n",
       "...          ...            ...  \n",
       "323580 -0.793904      -1.023541  \n",
       "323581 -0.393857      -0.082730  \n",
       "323582 -0.020503      -0.344498  \n",
       "323583 -0.417212       0.607781  \n",
       "323584 -0.863611      -0.372984  \n",
       "\n",
       "[323585 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload trees\n",
    "# dataset_path = 'serializedGraphs/'\n",
    "dataset_path = 'NewserializedGraphs/'\n",
    "files = os.listdir(dataset_path)\n",
    "\n",
    "# Initiate new df\n",
    "features = ['graph', 'node', 'node_text', 'tokenized_node_text', 'parent', 'parent_text', 'tokenized_parent_text', \n",
    "            'node_children','parent_children', 'num_node_sibilings', 'depth', 'votes', 'root_time_dif', 'parent_time_dif', \n",
    "            'comments_from_parent_to_child', 'edited', 'root_is_question', 'polarity', 'distance', 'root_distance']\n",
    "main_df = pd.DataFrame(columns=features)\n",
    "\n",
    "# create temporary df for each tree\n",
    "for file in files:\n",
    "    data = pkl.load(open(dataset_path + file, 'rb'))\n",
    "    df = pd.DataFrame(columns=features[:-2])\n",
    "    root_node = file.split('.')[0] + '.0'\n",
    "    root_text = data.nodes[root_node]['text']\n",
    "    question = 0\n",
    "    answers = []\n",
    "    if len(root_text) == 0:\n",
    "        root_node = list(data.predecessors(root_node))[0]\n",
    "        root_text = data.nodes[root_node]['text']\n",
    "    if '?' in root_text:\n",
    "        question = 1\n",
    "        answers = list(data.predecessors(root_node))\n",
    "    root_time = int(data.nodes[root_node]['created']/1000)\n",
    "    tokenized_root_text = prepare_text_for_d2v(root_text)\n",
    "    vote_scores = np.array([1, 2, 3, 4, 5]).reshape(-1,1)\n",
    "    # iterate through each node in the tree, ignoring the root nodes (0,1)\n",
    "    for i, idx in enumerate(data.nodes()):\n",
    "        # .0 node will have no parent, do not include in dataset\n",
    "        if '.0' in idx:\n",
    "            pass\n",
    "        # root node will have no parent, do not include in dataset\n",
    "        elif root_node == idx:\n",
    "            pass\n",
    "        # answers to debate questions have no polarity, do not include in dataset\n",
    "        elif idx in answers:\n",
    "            pass\n",
    "        # extract features for each node\n",
    "        else:\n",
    "            # graph id\n",
    "            graph = idx.split('.')[0]\n",
    "            # node text\n",
    "            node_text = data.nodes[idx]['text']\n",
    "            tokenized_node_text = prepare_text_for_d2v(node_text)\n",
    "            # parent id \n",
    "            parent = list(data.successors(idx))[0]\n",
    "            # parent text\n",
    "            parent_text = data.nodes[parent]['text']\n",
    "            tokenized_parent_text = prepare_text_for_d2v(parent_text)\n",
    "            # node children id\n",
    "            node_children = list(data.predecessors(idx))\n",
    "            # parent children id\n",
    "            parent_children = list(data.predecessors(parent))\n",
    "            # node number of sibilings\n",
    "            num_sibilings = len(parent_children) - 1\n",
    "            # node votes\n",
    "            votes = np.array(data.nodes[idx]['votes']).reshape(1,-1)\n",
    "            if votes.any() > 0:\n",
    "                average_vote = (votes @ vote_scores)[0,0] / votes.sum()\n",
    "            else:\n",
    "                average_vote = 0\n",
    "            # time dif root\n",
    "            node_time = int(data.nodes[idx]['created']/1000)\n",
    "\n",
    "            root_dif = datetime.utcfromtimestamp(node_time) - datetime.utcfromtimestamp(root_time)\n",
    "            # time dif parent\n",
    "            parent_time = int(data.nodes[parent]['created']/1000)\n",
    "            parent_dif = datetime.utcfromtimestamp(node_time) - datetime.utcfromtimestamp(parent_time)\n",
    "            # comment dif parent\n",
    "            comment_dif = int(idx.split('.')[1]) - int(parent.split('.')[1])\n",
    "            # binary edited column\n",
    "            if node_time == int(data.nodes[idx]['edited']/1000):\n",
    "                edited = 0\n",
    "            else:\n",
    "                edited = 1\n",
    "            # depth, do not consider actual root node\n",
    "            try:\n",
    "                depth = len(nx.shortest_path(data, source=idx, target=root_node)) - 1\n",
    "            # dealing with cyclical nodes that do not reach the root\n",
    "            except:\n",
    "                depth = 0\n",
    "            # polarity from child to parent\n",
    "            polarity = data.edges[idx, parent]['weight']\n",
    "\n",
    "            df.loc[i] = [graph, idx, node_text, tokenized_node_text, parent, parent_text, tokenized_parent_text, \n",
    "                         node_children, parent_children, num_sibilings, depth, average_vote, root_dif, parent_dif, \n",
    "                         comment_dif, edited, question, polarity]\n",
    "    # vecotrizing texts and computing distances between children and parents\n",
    "    texts = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['tokenized_node_text'])]\n",
    "    # build d2v model on entire corpus\n",
    "    if len(texts) == 0: \n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            d2v_model = d2v(texts, vector_size=50, alpha=.025, min_alpha=0.00025, workers=8, epochs=200)\n",
    "        except:\n",
    "            d2v_model = d2v(texts, vector_size=50, alpha=.025, min_alpha=0.00025, workers=8, epochs=200, min_count=1)\n",
    "\n",
    "        def get_vectors(model, texts):\n",
    "            vectors = [model.infer_vector(text, epochs=200) for text in texts]\n",
    "            return vectors\n",
    "\n",
    "        children_vecs = np.array(get_vectors(d2v_model, df['tokenized_node_text']))\n",
    "        parent_vecs = np.array(get_vectors(d2v_model, df['tokenized_parent_text']))\n",
    "        root_vec = d2v_model.infer_vector(tokenized_root_text, epochs=200)\n",
    "        distances = np.linalg.norm(children_vecs - parent_vecs, axis=1)\n",
    "        root_distances = np.linalg.norm(children_vecs - root_vec, axis=1)\n",
    "    #     norm_distances = min_max_scaler.fit_transform(distances.reshape(-1,1))\n",
    "        norm_distances = (root_distances - root_distances.mean()) / root_distances.std()\n",
    "        norm_root_distances = (distances - distances.mean()) / distances.std()\n",
    "        df['distance'] = norm_distances\n",
    "        df['root_distance'] = norm_root_distances\n",
    "    \n",
    "    # concat temp df to main df\n",
    "    main_df = pd.concat([main_df, df], ignore_index=True, axis=0)\n",
    "    \n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df.to_csv('nodes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import csvs for bert prediction scores and training set\n",
    "# scores_df = pd.read_csv('all_samples_prediction_score.csv')\n",
    "# train_samples_df = pd.read_csv('train_graph_set.csv')\n",
    "# # Add training set column to bert scores df\n",
    "# scores_df['training_set'] = 0\n",
    "\n",
    "# # ***Label if the bert scores belong to the training or test set***\n",
    "# for i in range(len(scores_df)):\n",
    "#     # Extract indexes from train samples df that have same child node text (can be multiple) \n",
    "#     matches = list(train_samples_df.index[train_samples_df['sentence2'] == scores_df.loc[i]['sentence2']])\n",
    "#     # If in the subset of train samples df there is the same parent node text, update row training_set label\n",
    "#     if scores_df.loc[i]['sentence1'] in list(train_samples_df.loc[matches]['sentence1']):\n",
    "#         scores_df.loc[i,'training_set'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csvs for bert predictions scores and training/test sets\n",
    "score_cols = ['score 0', 'score 1']\n",
    "\n",
    "train_baseline_scores_df = pd.read_csv('baseline_train_score.csv', usecols=score_cols)\n",
    "test_baseline_scores_df = pd.read_csv('baseline_test_score.csv', usecols=score_cols)\n",
    "\n",
    "train_walk_scores_df = pd.read_csv('biased_walk_train_score_new.csv', usecols=score_cols)\n",
    "test_walk_scores_df = pd.read_csv('biased_walk_test_score_new.csv', usecols=score_cols)\n",
    "\n",
    "train_samples = pd.read_csv('reference_train_set.csv', dtype={'node_id_1': str, 'node_id_2': str})\n",
    "test_samples = pd.read_csv('reference_test_set.csv', dtype={'node_id_1': str, 'node_id_2': str})\n",
    "# Label samples with train or test set\n",
    "train_baseline_scores_df['training_set'] = 1\n",
    "test_baseline_scores_df['training_set'] = 0\n",
    "# combine datasets\n",
    "baseline_scores_df = pd.concat([train_baseline_scores_df, test_baseline_scores_df], ignore_index=True, axis=0)\n",
    "\n",
    "walk_scores_df = pd.concat([train_walk_scores_df, test_walk_scores_df], ignore_index=True, axis=0)\n",
    "walk_scores_df.rename(columns={\"score 0\": \"score 0 walk\", \"score 1\": \"score 1 walk\"}, inplace=True)\n",
    "\n",
    "scores_df = pd.concat([baseline_scores_df, walk_scores_df], axis=1)\n",
    "samples_df = pd.concat([train_samples, test_samples], ignore_index=True, axis=0)\n",
    "scores_df = pd.concat([scores_df, samples_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import csvs for bert predictions scores and training/test sets\n",
    "# score_cols = ['score 0', 'score 1']\n",
    "# scores_df = pd.read_csv('biased_walk_test_score_new.csv', usecols=score_cols)\n",
    "# samples_df = pd.read_csv('reference_test_set.csv', dtype={'node_id_1': str, 'node_id_2': str})\n",
    "# scores_df['training_set'] = 0\n",
    "# # combine datasets\n",
    "# scores_df = pd.concat([scores_df, samples_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score 0</th>\n",
       "      <th>score 1</th>\n",
       "      <th>training_set</th>\n",
       "      <th>score 0 walk</th>\n",
       "      <th>score 1 walk</th>\n",
       "      <th>node_id_1</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>node_id_2</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324368</th>\n",
       "      <td>0.990763</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.359839</td>\n",
       "      <td>0.958298</td>\n",
       "      <td>16420.24</td>\n",
       "      <td>Politicians need to be trained, otherwise thei...</td>\n",
       "      <td>16420.45</td>\n",
       "      <td>Most sortition examples maintain Minister and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324369</th>\n",
       "      <td>0.525867</td>\n",
       "      <td>0.474133</td>\n",
       "      <td>0</td>\n",
       "      <td>0.662219</td>\n",
       "      <td>-0.541282</td>\n",
       "      <td>18056.9</td>\n",
       "      <td>Good Solution architects are in short supply, ...</td>\n",
       "      <td>18056.10</td>\n",
       "      <td>One needs to have a background in IT and busin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324370</th>\n",
       "      <td>0.157721</td>\n",
       "      <td>0.842279</td>\n",
       "      <td>0</td>\n",
       "      <td>0.701008</td>\n",
       "      <td>-0.587138</td>\n",
       "      <td>30792.15</td>\n",
       "      <td>Arguments that entities are bought off to say ...</td>\n",
       "      <td>30792.17</td>\n",
       "      <td>Bill Gates doesn't need the money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324371</th>\n",
       "      <td>0.921415</td>\n",
       "      <td>0.078586</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.303002</td>\n",
       "      <td>1.546477</td>\n",
       "      <td>22035.5</td>\n",
       "      <td>Tiger parenting teaches children to be discipl...</td>\n",
       "      <td>22035.217</td>\n",
       "      <td>A lack of self discipline may lead to failure,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324372</th>\n",
       "      <td>0.950283</td>\n",
       "      <td>0.049717</td>\n",
       "      <td>0</td>\n",
       "      <td>2.049757</td>\n",
       "      <td>-1.565363</td>\n",
       "      <td>2995.12</td>\n",
       "      <td>Standing minimizes the issue of police violenc...</td>\n",
       "      <td>2995.13</td>\n",
       "      <td>Standing implies that it is more important to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score 0   score 1  training_set  score 0 walk  score 1 walk  \\\n",
       "324368  0.990763  0.009237             0     -1.359839      0.958298   \n",
       "324369  0.525867  0.474133             0      0.662219     -0.541282   \n",
       "324370  0.157721  0.842279             0      0.701008     -0.587138   \n",
       "324371  0.921415  0.078586             0     -1.303002      1.546477   \n",
       "324372  0.950283  0.049717             0      2.049757     -1.565363   \n",
       "\n",
       "       node_id_1                                         sentence_1  \\\n",
       "324368  16420.24  Politicians need to be trained, otherwise thei...   \n",
       "324369   18056.9  Good Solution architects are in short supply, ...   \n",
       "324370  30792.15  Arguments that entities are bought off to say ...   \n",
       "324371   22035.5  Tiger parenting teaches children to be discipl...   \n",
       "324372   2995.12  Standing minimizes the issue of police violenc...   \n",
       "\n",
       "        node_id_2                                         sentence_2  label  \n",
       "324368   16420.45  Most sortition examples maintain Minister and ...      0  \n",
       "324369   18056.10  One needs to have a background in IT and busin...      1  \n",
       "324370   30792.17                  Bill Gates doesn't need the money      1  \n",
       "324371  22035.217  A lack of self discipline may lead to failure,...      1  \n",
       "324372    2995.13  Standing implies that it is more important to ...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7904707587014829"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = scores_df[scores_df['training_set'] == 0]\n",
    "scores = np.array(test_set[['score 0', 'score 1']])\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "y_test = test_set['label']\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new colums to -1 since there are nodes that are not included in the training or test set\n",
    "df['baseline_score_0'] = -1\n",
    "df['baseline_score_1'] = -1\n",
    "df['bert_score_0'] = -1\n",
    "df['bert_score_1'] = -1\n",
    "df['training_set'] = -1\n",
    "# set columns to type object to accept lists in cells\n",
    "df['baseline_score_0'] = df['baseline_score_0'].astype(object)\n",
    "df['baseline_score_1'] = df['baseline_score_1'].astype(object)\n",
    "df['bert_score_0'] = df['bert_score_0'].astype(object)\n",
    "df['bert_score_1'] = df['bert_score_1'].astype(object)\n",
    "df['training_set'] = df['training_set'].astype(object)\n",
    "# set column as type float to compare with scores df\n",
    "# df['node'] = df['node'].astype(float)\n",
    "\n",
    "# ***Add bert score and training set label to main df***\n",
    "for i in range(len(df)):\n",
    "    # Get loc index for replacing values\n",
    "    name = df.iloc[i].name \n",
    "    # Extract indexes from bert scores df that have same child node id \n",
    "    matches = list(scores_df.index[scores_df['node_id_2'] == df.loc[i]['node']])\n",
    "    # Transfer single value if only one match\n",
    "    if len(matches) == 1:\n",
    "        idx = matches[0]\n",
    "        df.loc[i,'baseline_score_0'] = scores_df.loc[idx, 'score 0']\n",
    "        df.loc[i,'baseline_score_1'] = scores_df.loc[idx, 'score 1']\n",
    "        df.loc[i,'bert_score_0'] = scores_df.loc[idx, 'score 0 walk']\n",
    "        df.loc[i,'bert_score_1'] = scores_df.loc[idx, 'score 1 walk']\n",
    "        df.loc[i, 'training_set'] = scores_df.loc[idx, 'training_set']\n",
    "    # Transfer array of values if multiple matches\n",
    "    elif len(matches) > 1:\n",
    "        df.at[i, 'baseline_score_0'] = np.array(scores_df.loc[matches]['score 0'])\n",
    "        df.at[i, 'baseline_score_1'] = np.array(scores_df.loc[matches]['score 1'])\n",
    "        df.at[i, 'bert_score_0'] = np.array(scores_df.loc[matches]['score 0 walk'])\n",
    "        df.at[i, 'bert_score_1'] = np.array(scores_df.loc[matches]['score 1 walk'])\n",
    "        df.at[i, 'training_set'] = np.array(scores_df.loc[matches]['training_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('nodes_with_bert_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph</th>\n",
       "      <th>node</th>\n",
       "      <th>node_text</th>\n",
       "      <th>tokenized_node_text</th>\n",
       "      <th>parent</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>tokenized_parent_text</th>\n",
       "      <th>node_children</th>\n",
       "      <th>parent_children</th>\n",
       "      <th>num_node_sibilings</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_score_1</th>\n",
       "      <th>training_set</th>\n",
       "      <th>node_pos_sentiment</th>\n",
       "      <th>node_neu_sentiment</th>\n",
       "      <th>node_neg_sentiment</th>\n",
       "      <th>node_comp_sentiment</th>\n",
       "      <th>parent_pos_sentiment</th>\n",
       "      <th>parent_neu_sentiment</th>\n",
       "      <th>parent_neg_sentiment</th>\n",
       "      <th>parent_comp_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.136</td>\n",
       "      <td>Many of the Western World countries have divor...</td>\n",
       "      <td>[many, the, western, world, country, have, div...</td>\n",
       "      <td>10035.43</td>\n",
       "      <td>'[Optimism bias](https://www.sciencedirect.com...</td>\n",
       "      <td>[optimism, URL, science, article, pii, s096098...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10035.136]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.04984</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.137</td>\n",
       "      <td>This argument does not take into account wheth...</td>\n",
       "      <td>[this, argument, doe, not, take, into, account...</td>\n",
       "      <td>10035.83</td>\n",
       "      <td>The fact that women are more likely to achieve...</td>\n",
       "      <td>[the, fact, that, woman, are, more, likely, ac...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10035.137, 10035.84, 10035.149]</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.551</td>\n",
       "      <td>1</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.133</td>\n",
       "      <td>Life is a constantly changing experience, and ...</td>\n",
       "      <td>[life, constantly, change, experience, and, it...</td>\n",
       "      <td>10035.93</td>\n",
       "      <td>Many people make fundamental life-altering cha...</td>\n",
       "      <td>[many, people, make, fundamental, life, alteri...</td>\n",
       "      <td>[10035.148]</td>\n",
       "      <td>[10035.133, 10035.122]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.740957</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.7783</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.232</td>\n",
       "      <td>Marriage should be treated like a business par...</td>\n",
       "      <td>[marriage, should, treat, like, business, part...</td>\n",
       "      <td>10035.107</td>\n",
       "      <td>As marriage loses ground in the modern world, ...</td>\n",
       "      <td>[marriage, lose, ground, the, modern, world, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10035.232, 10035.110, 10035.168, 10035.170, 1...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947038</td>\n",
       "      <td>1</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10035</td>\n",
       "      <td>10035.8</td>\n",
       "      <td>Prenuptial agreements can protect individuals ...</td>\n",
       "      <td>[prenuptial, agreement, can, protect, individu...</td>\n",
       "      <td>10035.1</td>\n",
       "      <td>All couples should sign a prenuptial agreement...</td>\n",
       "      <td>[all, couple, should, sign, prenuptial, agreem...</td>\n",
       "      <td>[10035.38, 10035.14, 10035.11, 10035.146, 1003...</td>\n",
       "      <td>[10035.8, 10035.4, 10035.5, 10035.6, 10035.7, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656902</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   graph       node                                          node_text  \\\n",
       "0  10035  10035.136  Many of the Western World countries have divor...   \n",
       "1  10035  10035.137  This argument does not take into account wheth...   \n",
       "2  10035  10035.133  Life is a constantly changing experience, and ...   \n",
       "3  10035  10035.232  Marriage should be treated like a business par...   \n",
       "4  10035    10035.8  Prenuptial agreements can protect individuals ...   \n",
       "\n",
       "                                 tokenized_node_text     parent  \\\n",
       "0  [many, the, western, world, country, have, div...   10035.43   \n",
       "1  [this, argument, doe, not, take, into, account...   10035.83   \n",
       "2  [life, constantly, change, experience, and, it...   10035.93   \n",
       "3  [marriage, should, treat, like, business, part...  10035.107   \n",
       "4  [prenuptial, agreement, can, protect, individu...    10035.1   \n",
       "\n",
       "                                         parent_text  \\\n",
       "0  '[Optimism bias](https://www.sciencedirect.com...   \n",
       "1  The fact that women are more likely to achieve...   \n",
       "2  Many people make fundamental life-altering cha...   \n",
       "3  As marriage loses ground in the modern world, ...   \n",
       "4  All couples should sign a prenuptial agreement...   \n",
       "\n",
       "                               tokenized_parent_text  \\\n",
       "0  [optimism, URL, science, article, pii, s096098...   \n",
       "1  [the, fact, that, woman, are, more, likely, ac...   \n",
       "2  [many, people, make, fundamental, life, alteri...   \n",
       "3  [marriage, lose, ground, the, modern, world, w...   \n",
       "4  [all, couple, should, sign, prenuptial, agreem...   \n",
       "\n",
       "                                       node_children  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                        [10035.148]   \n",
       "3                                                 []   \n",
       "4  [10035.38, 10035.14, 10035.11, 10035.146, 1003...   \n",
       "\n",
       "                                     parent_children num_node_sibilings  ...  \\\n",
       "0                                        [10035.136]                  0  ...   \n",
       "1                   [10035.137, 10035.84, 10035.149]                  2  ...   \n",
       "2                             [10035.133, 10035.122]                  1  ...   \n",
       "3  [10035.232, 10035.110, 10035.168, 10035.170, 1...                  6  ...   \n",
       "4  [10035.8, 10035.4, 10035.5, 10035.6, 10035.7, ...                 14  ...   \n",
       "\n",
       "  bert_score_1 training_set node_pos_sentiment node_neu_sentiment  \\\n",
       "0      2.04984            0              0.000              1.000   \n",
       "1       -3.551            1              0.057              0.833   \n",
       "2    -0.740957            1              0.000              0.865   \n",
       "3     0.947038            1              0.221              0.703   \n",
       "4     1.656902            1              0.278              0.533   \n",
       "\n",
       "  node_neg_sentiment node_comp_sentiment parent_pos_sentiment  \\\n",
       "0              0.000              0.0000                0.000   \n",
       "1              0.110             -0.2960                0.000   \n",
       "2              0.135             -0.7783                0.132   \n",
       "3              0.076              0.8519                0.303   \n",
       "4              0.189              0.1280                0.314   \n",
       "\n",
       "  parent_neu_sentiment  parent_neg_sentiment  parent_comp_sentiment  \n",
       "0                1.000                 0.000                 0.0000  \n",
       "1                1.000                 0.000                 0.0000  \n",
       "2                0.868                 0.000                 0.4939  \n",
       "3                0.614                 0.083                 0.6249  \n",
       "4                0.686                 0.000                 0.4939  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing for sentiment, better to do after bert scores added\n",
    "df = df.drop(index=df[df['polarity'] == 0].index) # drop nodes connected to root\n",
    "df = df.drop(index=df[df['node_text'].isnull()].index) # drop nodes with no text\n",
    "df = df.drop(index=df[df['parent_text'].isnull()].index) # drop nodes with no parent text\n",
    "\n",
    "# Initialize pretrained sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Extract each sentiment score for child node\n",
    "all_scores = df['node_text'].apply(lambda text: sid.polarity_scores(text))\n",
    "df['node_pos_sentiment']  = all_scores.apply(lambda score_dict: score_dict['pos'])\n",
    "df['node_neu_sentiment']  = all_scores.apply(lambda score_dict: score_dict['neu'])\n",
    "df['node_neg_sentiment']  = all_scores.apply(lambda score_dict: score_dict['neg'])\n",
    "df['node_comp_sentiment']  = all_scores.apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "# Extract each sentiment score for parent node\n",
    "all_scores = df['parent_text'].apply(lambda text: sid.polarity_scores(text))\n",
    "df['parent_pos_sentiment']  = all_scores.apply(lambda score_dict: score_dict['pos'])\n",
    "df['parent_neu_sentiment']  = all_scores.apply(lambda score_dict: score_dict['neu'])\n",
    "df['parent_neg_sentiment']  = all_scores.apply(lambda score_dict: score_dict['neg'])\n",
    "df['parent_comp_sentiment']  = all_scores.apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "# df.to_csv('nodes_and_sentiment.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Tokenization\n",
    "# parser = English()\n",
    "\n",
    "# def tokenize(text):\n",
    "#     lda_tokens = []\n",
    "#     tokens = parser(text)\n",
    "#     for token in tokens:\n",
    "#         if token.orth_.isspace():\n",
    "#             continue\n",
    "#         elif token.like_url:\n",
    "#             lda_tokens.append('URL')\n",
    "#         elif token.orth_.startswith('@'):\n",
    "#             lda_tokens.append('SCREEN_NAME')\n",
    "#         else:\n",
    "#             lda_tokens.append(token.lower_)\n",
    "#     return lda_tokens\n",
    "\n",
    "# # Define word root process\n",
    "# def get_lemma(word):\n",
    "#     # get root\n",
    "#     lemma = wn.morphy(word)\n",
    "#     if lemma is None:\n",
    "#         return word\n",
    "#     else:\n",
    "#         return lemma\n",
    "    \n",
    "# # Define Stopwords\n",
    "# en_stop = list(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# # preprocessing function utilizing tokenization, extracting root words, and removing stop words\n",
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 2]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "    \n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply tokenization function on child node text data\n",
    "# texts = df['node_text']\n",
    "# text_data = []\n",
    "\n",
    "\n",
    "# for doc in texts:\n",
    "#     tokens = prepare_text_for_lda(str(doc))\n",
    "#     text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionary and corpus for topic model\n",
    "# dictionary = corpora.Dictionary(text_data)\n",
    "# corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Topic models with different k values\n",
    "# lda_2 = gensim.models.ldamodel.LdaModel(corpus, num_topics = 2, id2word=dictionary, passes=15)\n",
    "# lda_12 = gensim.models.ldamodel.LdaModel(corpus, num_topics = 12, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extracting topic with highest probability for each node\n",
    "# all_topics_2 = lda_2.get_document_topics(corpus, minimum_probability=0.0)\n",
    "# all_topics_csr_2 = gensim.matutils.corpus2csc(all_topics_2)\n",
    "# all_topics_numpy_2 = all_topics_csr_2.T.toarray()\n",
    "# labels_2 = np.argmax(all_topics_numpy_2, axis=1)\n",
    "\n",
    "# all_topics_12 = lda_12.get_document_topics(corpus, minimum_probability=0.0)\n",
    "# all_topics_csr_12 = gensim.matutils.corpus2csc(all_topics_12)\n",
    "# all_topics_numpy_12 = all_topics_csr_12.T.toarray()\n",
    "# labels_12 = np.argmax(all_topics_numpy_12, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df['node_topic'] = labels_2\n",
    "# # df['parent_topic'] = p_labels_2\n",
    "\n",
    "# df['node_topic'] = labels_12\n",
    "# df['parent_topic'] = p_labels_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('nodes_with_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nodes_with_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('nodes_with_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "# dealing with multiple matches in training_set label and bert score\n",
    "for i in range(len(df2)):\n",
    "    name = df2.iloc[i].name\n",
    "    training_set = df2.iloc[i]['training_set']\n",
    "    baseline_score_0 = df2.iloc[i]['baseline_score_0']\n",
    "    baseline_score_1 = df2.iloc[i]['baseline_score_1']\n",
    "    bert_score_0 = df2.iloc[i]['bert_score_0']\n",
    "    bert_score_1 = df2.iloc[i]['bert_score_1']\n",
    "    # if there is a match in the training set, label as such and use that associated bert score\n",
    "    if type(training_set) is np.ndarray:\n",
    "        idx = training_set.argmax()\n",
    "        df2.loc[name, 'training_set'] = training_set[idx]\n",
    "        df2.loc[name, 'baseline_score_0'] = baseline_score_0[idx]\n",
    "        df2.loc[name, 'baseline_score_1'] = baseline_score_1[idx]\n",
    "        df2.loc[name, 'bert_score_0'] = bert_score_0[idx]\n",
    "        df2.loc[name, 'bert_score_1'] = bert_score_1[idx]\n",
    "\n",
    "# change labels and data types for XGBoost model\n",
    "df2['polarity'].replace({-1:0}, inplace=True)\n",
    "df2['parent_time_dif'] = df2['parent_time_dif'].apply(lambda x: x.days)\n",
    "df2['root_time_dif'] = df2['root_time_dif'].apply(lambda x: x.days)\n",
    "object_columns = ['num_node_sibilings', 'votes', 'comments_from_parent_to_child', 'edited', 'root_is_question', \n",
    "                  'distance', 'root_distance', 'depth', 'baseline_score_0', 'baseline_score_1', \n",
    "                  'bert_score_0', 'bert_score_1']\n",
    "for col in object_columns:\n",
    "    df2[col] = pd.to_numeric(df2[col])\n",
    "\n",
    "# Extract desired features \n",
    "# features = ['bert_score_0', 'bert_score_1', 'num_node_sibilings', 'depth', 'votes', 'root_time_dif', \n",
    "#             'parent_time_dif', 'comments_from_parent_to_child', 'edited', 'root_is_question', 'distance', 'root_distance',\n",
    "#             'node_pos_sentiment', 'parent_pos_sentiment', 'node_neg_sentiment', 'parent_neg_sentiment',\n",
    "#             'training_set', 'polarity']\n",
    "features = ['num_node_sibilings', 'depth', 'votes', 'root_time_dif', \n",
    "            'parent_time_dif', 'comments_from_parent_to_child', 'root_is_question', 'distance', 'root_distance',\n",
    "            'node_pos_sentiment', 'node_neg_sentiment', 'parent_neg_sentiment',\n",
    "            'training_set', 'polarity']\n",
    "baseline_features = ['baseline_score_0', 'baseline_score_1'] + features\n",
    "bert_features = ['bert_score_0', 'bert_score_1'] + features\n",
    "\n",
    "baseline_df = df2[baseline_features]\n",
    "bert_df = df2[bert_features]\n",
    "\n",
    "# Split data, ignore nodes that do not appear in training or test sets\n",
    "baseline_train, bert_train = baseline_df[baseline_df['training_set']==1], bert_df[bert_df['training_set']==1]\n",
    "baseline_test, bert_test = baseline_df[baseline_df['training_set']==0], bert_df[bert_df['training_set']==0]\n",
    "\n",
    "baseline_X_train, baseline_y_train = baseline_train[baseline_features[:-2]], baseline_train['polarity']\n",
    "bert_X_train, bert_y_train = bert_train[bert_features[:-2]], bert_train['polarity']\n",
    "\n",
    "baseline_X_test, baseline_y_test = baseline_test[baseline_features[:-2]], baseline_test['polarity']\n",
    "bert_X_test, bert_y_test = bert_test[bert_features[:-2]], bert_test['polarity']\n",
    "\n",
    "# X_train, y_train = train[['node_comp_sentiment', 'parent_comp_sentiment']], train['polarity']\n",
    "# X_test, y_test = test[['node_comp_sentiment', 'parent_comp_sentiment']], test['polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Scores Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7908325856254448"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.array(baseline_X_test[['baseline_score_0', 'baseline_score_1']])\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(baseline_y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline XGBoost Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8035797159741345"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# train\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(baseline_X_train, baseline_y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(baseline_X_test)\n",
    "\n",
    "accuracy = accuracy_score(baseline_y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEGCAYAAAAE8QIHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcpklEQVR4nO3deZxcZZ3v8c+3lyRkIZB0ICELBIEgLgRuCIIsAQUC6kVGRgPO4CgMhDF69aoj6gzewX0cX+qFYIxMBh0Hg46oYYiEuSgCypKAbAkG2iBJZzHpBLISevvdP6o6qXS6q+qQqq6q09/363VerzrnPPWcp2n6l+c5z6aIwMwsLeoqXQAzs1JyUDOzVHFQM7NUcVAzs1RxUDOzVGmodAFyNY2qj6MmNla6GJbAc08NrXQRLIHd7KQtXtWB5HHBOcNi85bOotI+9tSrSyJi5oE8L6mqCmpHTWzk0SUTK10MS+CCI6ZWugiWwCNx7wHn0bqlk0eWTCgqbeO4PzYd8AMTqqqgZma1IOiMrkoXok8OamaWSABdVO+gfQc1M0usC9fUzCwlgqDdzU8zS4sAOt38NLM08Ts1M0uNADqreHUfBzUzS6x636g5qJlZQkH4nZqZpUcEtFdvTHNQM7OkRCcHNH20rBzUzCyRALqquKbmpYfMLLHObG2t0FGIpJmSVkpqlnRdL/dHSrpT0pOSlkv6YKE8XVMzs0Qyg28PvPkpqR6YC5wHtABLJS2KiBU5yT4MrIiId0kaA6yU9B8R0dZXvg5qZpZIAO1RkkbedKA5IlYBSFoIXAzkBrUARkgSMBzYAnTky9RBzcwSCURnad5cjQfW5Jy3AKf2SHMTsAhYB4wA3heRf+Kp36mZWWJdoaIOoEnSspzj6pxsemvD9uyCuAB4AjgCmArcJOngfGVzTc3MEkn4Tq01Iqb1ca8FyF3qegKZGlmuDwJfjcyu682SXgCOBx7t64GuqZlZQqIz6oo6ClgKHCtpsqRBwCwyTc1cq4G3AUg6HJgCrMqXqWtqZpZIZuXbA68PRUSHpDnAEqAeWBARyyXNzt6fB3wBuFXS02Saq5+OiNZ8+TqomVkiEaIt6kuUVywGFve4Ni/n8zrg/CR5OqiZWWJdniZlZmmR6Sio3tfxDmpmlpCK6QSoGAc1M0ukVB0F5eKgZmaJdYbfqZlZSgSiPao3dFRvycysKrmjwMxSJZCbn2aWLu4oMLPUiMBDOswsPTIdBaWZJlUODmpmlpg7CswsNYI9C0BWJQc1M0vMNTUzS43Mvp8OamaWGt6h3cxSJLNFnns/zSwlIuTmp5mliwffmllqZNZT8zs1M0sNr3xrZimSGdJRvTW16g23ZlaVuud+FnMUImmmpJWSmiVd18v9T0l6Ins8I6lT0qh8eTqomVliXdQVdeQjqR6YC1wInABcJumE3DQR8fWImBoRU4HPAL+JiC358nXz08wSySw9VJLm53SgOSJWAUhaCFwMrOgj/WXAjwpl6pqamSXWFSrqAJokLcs5rs7JZjywJue8JXttP5KGAjOBnxYqm2tqZpZIZpWOoutDrRExrY97vVX3oo+07wJ+W6jpCQ5qZpZQZppUSRp5LcDEnPMJwLo+0s6iiKYnuPl5QJb+egRXnnE8f3P667n9xsP2u79zWx3XXzGZ2W+fwt/OmMKShXs7bXZsrecLf3sUV555PFeddTwrlg3tz6IPWNNmbOOWB/7Av/32Wd4758/73Z94zG6+ueh57nzhKS6dvXG/+3V1wdx7VnLD91f1R3GrVKamVsxRwFLgWEmTJQ0iE7gW7fc0aSRwNvCLYkpX1pqapJnAt4F64JaI+Go5n9efOjth7mcn8JWFf6RpXDsfueg43nLBVo487tU9aRbd2sSk43Zzww9e4OXN9Vx55us59y9eonFQ8J3rxzNtxjb+8Xt/or1NvPqK/30pt7q64MNfXstnZh1N6/pGblz8PA8vGcnq54fsSbPtpXq+84/jOX3m1l7zePdVrax5fghDh3f2V7GrUilmFEREh6Q5wBIyMWJBRCyXNDt7f1426SXAPRGxs5h8y/aXVEx3bS1b+fuhHHHUq4w7so3GQcGMi1/ioSUj90kjwSs764mA3TvrGXFIJ/UNwc7tdTz98DBmXp55PdA4KBg+cmD/kfSHKSftYt2fBrFh9WA62uu47xeHcNoF+wavrZsbee7JoXR07P9H2zSujelv28Yvb8s7TCr1uns/izkK5xWLI+K4iHhdRHwpe21eTkAjIm6NiFnFlq+c1YM93bUR0QZ0d9emwuYNjYw5on3PedO4dlrXN+6T5n9+sJXVzw/m8pPewDXnTuHaG9ZSVwcbXhzMyNEdfOPjk/i7847jm5+YyO5drqmV2+ix7WxaN2jPeev6RprGtef5xr5m/9M6bvniOKKrekfT95cSNT/LopxPLaq7VtLV3d29mzbXTm0leumjUY//1x+7bwSve8Mr3Pb75dz83yuZ+7nx7NxeR2cnND89lHde0crN//0cQ4Z2cftN+7+Ts9Lq+fuB3n+PvTn17dt4ubWB5qf97rN7j4Iih3T0u3IGtaK6ayNifkRMi4hpY0ZX78JzPTWNa2fTur01s9b1jYweu++/+vfcPoq3XrQVCcZPbmPspDbWNA+haVw7Y8a1c/zJuwA4450v0/z0Qf1a/oGodX0jY45o23PeNK6dzRsa83xjrxNO2clbzt/G9x9ZwWe+8yInnrGDv7/xxXIVtaoF0BF1RR2VUM6nJumurTlTpu5i7QuD2bB6EO1t4r5fHMpbzt+2T5ox49t54oERALy0qYGWPw5m3KRXGXVYB01HtLGmeTAATzwwgknHvrrfM6y0Vj4xlPGT2zh84qs0NHYx4+KXefiekYW/CPzbV8bxV9NO4AOnnsBXrj2SJx8czj9/5Mgyl7h6VXPzs5y9n3u6a4G1ZLprLy/j8/pVfQN8+EstfPbyo+nqFOfP2sJRU3bzXz8YDcA7r9jM+z+2gX/52CSuOXcKEXDl59YzcnSmif3hL67la3OOpKNdjJ3Uxie+ubqSP86A0NUp5n5uPF++bRV19XDPwlG8+NwQ3vHXrQDc9e9NHDqmnRt/+TxDR3QSXZnezqtnTGHXjtppRZRdBZuWxVAU+1LhtWQuXQR8i73dtV/Kl37aiUPi0SUT8yWxKnPBEVMrXQRL4JG4l22x5YAi0qHHHxbnLri0qLR3vPU7j+WZUVAWZR2nFhGLgcXlfIaZ9b9qrql5mpSZJVLti0Q6qJlZIoHo6KrecZUOamaWmDdeMbP0CDc/zSxF/E7NzFLHQc3MUiMQne4oMLM0cUeBmaVGuKPAzNImHNTMLD2qe0K7g5qZJeaampmlRgR0VvGS5g5qZpZYNfd+Vu9gEzOrSkGm+VnMUYikmZJWSmqWdF0faWZIekLSckm/KZSna2pmllBpOgpyttE8j8zy/0slLYqIFTlpDgFuBmZGxGpJBXcock3NzBKLKO4ooJhtNC8H7oiI1ZnnxsZCmTqomVliJWp+FrON5nHAoZLuk/SYpCsKZermp5klkun9LLo+1CRpWc75/IiYn/1czDaaDcD/AN4GHAQ8JOnhiHiurwc6qJlZYgn2a2rNs/FKMdtotmTz2AnslHQ/cCLQZ1Bz89PMEitR83PPNpqSBpHZRnNRjzS/AM6U1CBpKHAq8Gy+TF1TM7NEguKGaxTMJ6JD0hxgCXu30VwuaXb2/ryIeFbS3cBTQBdwS0Q8ky9fBzUzS6xUuwX3to1mRMzrcf514OvF5umgZmbJBISnSZlZmnhCu5mlSoLez37XZ1CTdCN5ms4R8dGylMjMqlr33M9qla+mtizPPTMbqAKoxaAWEd/PPZc0LDsAzswGuGpufhYcfCvpNEkryA54k3SipJvLXjIzq1Iiuoo7KqGYGQXfAi4ANgNExJPAWWUsk5lVuyjyqICiej8jYo20T9TtLE9xzKzqRe12FHRbI+l0ILLzsz5KgblXZpZytfxODZgNfJjMOkdrganZczMbsFTk0f8K1tQiohV4fz+UxcxqRVelC9C3Yno/j5Z0p6RNkjZK+oWko/ujcGZWhbrHqRVzVEAxzc/bgB8D44AjgJ8APypnocysupVoj4KyKCaoKSL+PSI6sscPqerXhGZWdrU4pEPSqOzHX2f341tIppjvA+7qh7KZWbWq0SEdj5EJYt2lvybnXgBfKFehzKy6qYrbavnmfk7uz4KYWY0IQa0vEinpjcAJwJDuaxHxg3IVysyqXC3W1LpJ+jwwg0xQWwxcCDwIOKiZDVRVHNSK6f28lMxGohsi4oNk9twbXNZSmVl1q8XezxyvRESXpA5JBwMbAQ++NRuoqnyRyGJqasskHQJ8j0yP6OPAo+UslJlVN0VxR8F8pJmSVkpqzg4d63l/hqStkp7IHtcXyrOYuZ9/l/04L7up6MER8VTh4ppZapWgaSmpHpgLnAe0AEslLYqIFT2SPhAR7yw233yDb0/Ody8iHi/2IWaWLiUapzYdaI6IVQCSFgIXAz2DWiL5amrfyHMvgHMP5MG9ee6Pozn/0g+UOlsro5nPPFDpIlgCf3hvidZ3Lf6dWpOk3E2c5kfE/Ozn8cCanHstwKm95HGapCeBdcAnI2J5vgfmG3x7TnFlNrMBJVnPZmtETOvjXm+RsWfOjwNHRsQOSRcBPweOzffAYjoKzMz2VZohHS3AxJzzCWRqY3sfE7EtInZkPy8GGiU15cvUQc3MElNXcUcBS4FjJU3ObhUwC1i0z3OkscpukCJpOpmYtTlfpkVNkzIz20cJOgoiokPSHGAJUA8siIjlkmZn788jM/j/WkkdwCvArIj8K7UVM01KZJbzPjoibpA0CRgbER6rZjYAFTsGrRjZJuXiHtfm5Xy+CbgpSZ7FND9vBk4DLsuebycztsTMBqoqXs67mObnqRFxsqTfA0TES9n2r5kNVFU8ob2YoNaeHfkbAJLGUNV7yZhZudXkIpE5/i/wM+AwSV8i8+LuH8paKjOrXlFUz2bFFDP38z8kPUZm+SEB744I79BuNpDVck0t29u5C7gz91pErC5nwcysitVyUCOzc1T3BixDgMnASuANZSyXmVWxmn6nFhFvyj3Prt5xTR/JzcwqKvGMgoh4XNIp5SiMmdWIWq6pSfrfOad1wMnAprKVyMyqW633fgIjcj53kHnH9tPyFMfMakKt1tSyg26HR8Sn+qk8ZlblRI12FEhqyM6i73NZbzMboGoxqJHZMepk4AlJi4CfADu7b0bEHWUum5lVoxKu0lEOxbxTG0VmUbZz2TteLQAHNbOBqkY7Cg7L9nw+w95g1q2K47SZlVut1tTqgeEUtzmCmQ0kVRwB8gW19RFxQ7+VxMxqQ7LdpPpdvqBWmWUrzazq1Wrz8239Vgozqy21GNQiYkt/FsTMakc1T5Pyvp9mlkyxGxkXUZuTNFPSSknNkq7Lk+4USZ2SLi2Up4OamSWiBEfefDLTMOcCFwInAJdJOqGPdF8jsz9oQQ5qZpZcaWpq04HmiFgVEW3AQuDiXtJ9hMwiGhuLKZqDmpkl1r2hcaGjgPHAmpzzluy1vc+RxgOXAPMoUuJFIs3MEvR+NklalnM+PyLmZz8XM7D/W8CnI6JTKm6UmYOamSWTbJHI1oiY1se9FmBizvkEYF2PNNOAhdmA1gRcJKkjIn7e1wMd1MwsudKMU1sKHCtpMrAWmAVcvs9jIiZ3f5Z0K/Bf+QIaOKiZ2WtQihkF2fUa55Dp1awHFkTEckmzs/eLfo+Wy0HNzJIr0YyCiFgMLO5xrddgFhF/U0yeDmpmllitzv00M9tfULOLRJqZ7admN14xM+uTg5qZpYmieqOag5qZJVPDK9+amfXK79TMLFWqeZFIBzUzS841NTNLjRTs0G5mti8HNTNLCw++NbPUUVf1RjUHNTNLxuPU0mva1LVc+8Gl1NUFd997DLf//E373D/3zFW8993PAPDK7kZunH8qq14cxYQjtvK5j9+/J93Yw3fwg9tP5Gd37beRjpXYpgfr+cNXhxCdYsJ72jj6qrZ97r+wYBDr72oEIDphx6o6znlgO/VD4NEPDKOrLXN97HkdHDPn1Ur8CFVhQA7pkLQAeCewMSLeWK7nVEpdXRdzrnqE6244j9YtQ7nxq4t5aNlEVrccsifNho3D+eT1F7Bj52BOOWktH5v9MB/9zEW0rBvJtZ961558bvvuf/LbRyZV6CcZOKITnv3iQUz73k6GjA0eet8wDjung+Gv2/sXOvlDbUz+UCbQbbyvgRd/MIhBIyECTlmwk4ah0NUOj14xjKYzOzjkxM5K/TiVVcU1tXLuJnUrMLOM+VfUlGM2s27DCDZsHEFHRz2/+e1RnH7Kmn3SrFh5GDt2Dgbg2eeaaBq1c798TnrTBtb/eQQbW4f3S7kHsq1P1zN0UhdDJwZ1jTDuwnY2/qrvf9fXL25k7EXtAEjQMDRzPTqgq4PCG1umWIl2kyqLsgW1iLgf2FKu/CutadQuNrUO23O+afNQRo/a1Wf6mW9rZunvx+93/ey3vsCvH5zcyzes1HZvFEPG7q2VDTk82L2x9z+Bzleg9cEGDj+vfc+16ITfvWcYvz5rBKNP6+CQNw/gWlpEcUcFVHzfT0lXS1omaVl7x/41marVyz9Dff0OT3zDBmae28wtPzx5n+sNDZ2cNq2F+x86shwltJ56+f30tevaxvsaOPSkDgaNzElbD6f/dCdn37udrU/Xs/35iv/5VIy6ijsqoeK/lYiYHxHTImJaY8Owwl+oEq2bhzGmaW8QHjN6F1teGrpfuslHvsTHr/0dn//aOWzfMWSfe6ectJbmF0bx8taDyl5ey9bMNuz9X373n8XgMb3/5W345d6mZ0+NB8OoUzpofXBg9rN1j1MbcM3PtFvZPJrx47Yz9rDtNDR0cvZb/8RDSyfuk2ZM0w6u/+R9/PONZ7B2/cH75XHOGX9y07MfHfzGTnatrmNXi+hqh/W/bOSwczr2S9e+HbYsa9jnXtsW0b4t87lzN2x+uIFhk6u4C7Ccim16Vqj5OTD/qSmBrq46brplOl/+h/9HXV2w5FfH8GLLIbzj/JUA3HXPFP7q0qc4eMSrfOSqRwDo7KpjzqffAcDgQR2c/OZ1fOu7b6nYzzDQ1DXA6z+7m8euGUp0ivGXtDH8mC7W3J4ZwjHxfZma2cZ7G2k6vWNPxwDAq5vE058bSnQCAYdf0M5hM/YPiANFNc8oUJQpmkr6ETCDzK7KfwY+HxH/mu87Bw8fH9OnXluW8lh5zJz/QKWLYAl8+70Ps+aZrQfUbzvikAlx0ln/q6i0D9z594/l2aEdSTOBb5PZ9/OWiPhqj/sXA18gs9VLB/CxiHgw3zPLVlOLiMvKlbeZVVYpamqS6oG5wHlAC7BU0qKIWJGT7F5gUUSEpDcDPwaOz5evm59mlkwAnSVp4U0HmiNiFYCkhcDFwJ6gFhE7ctIPo4hhv+4oMLPEEvR+NnUP2coeV+dkMx7IHbHekr2277OkSyT9AbgL+FChsrmmZmbJFf8uvjXPO7Xe3u3tl3FE/Az4maSzyLxfe3u+B7qmZmaJlWicWguQOw5qArCur8TZWUqvk9SUL1MHNTNLJhIc+S0FjpU0WdIgYBawKDeBpGOkzLwPSScDg4DN+TJ189PMEhGgEnQURESHpDnAEjJDOhZExHJJs7P35wHvAa6Q1A68ArwvCoxDc1Azs8RKtUN7RCwGFve4Ni/n89eAryXJ00HNzJLxyrdmli6Vm9dZDAc1M0usmud+OqiZWXKuqZlZakRpej/LxUHNzJKr3pjmoGZmyZVqSEc5OKiZWXIOamaWGkFmycYq5aBmZomIcPPTzFKmq3qrag5qZpaMm59mljZufppZujiomVl6eEK7maVJ6XaTKgsHNTNLzO/UzCxdHNTMLDUC6HJQM7PUcEeBmaVNFQc17/tpZskE0NlV3FGApJmSVkpqlnRdL/ffL+mp7PE7SScWytM1NTNLKCAOfJ6UpHpgLnAemd3al0paFBErcpK9AJwdES9JuhCYD5yaL18HNTNLrjTNz+lAc0SsApC0ELgY2BPUIuJ3OekfBiYUytTNTzNLprv3s5gjv/HAmpzzluy1vlwJ/LJQpq6pmVlyxdfUmiQtyzmfHxHzs5/VW869ZSLpHDJB7YxCD3RQM7Pkig9qrRExrY97LcDEnPMJwLqeiSS9GbgFuDAiNhd6oIOamSUTAZ2dpchpKXCspMnAWmAWcHluAkmTgDuAv46I54rJ1EHNzJIrQUdBRHRImgMsAeqBBRGxXNLs7P15wPXAaOBmSQAdeWp+gIOamb0WJRp8GxGLgcU9rs3L+XwVcFWSPB3UzCyhono2K8ZBzcySCYgSDL4tFwc1M0uuiClQleKgZmbJRHiLPDNLmSpepcNBzcwSC9fUzCw9vEikmaWJl/M2szQJIEozTaosHNTMLJkozSKR5eKgZmaJhZufZpYqVVxTU1RRL4akTcCLlS5HGTQBrZUuhCWS1t/ZkREx5kAykHQ3mf8+xWiNiJkH8rykqiqopZWkZYWWS7Hq4t9Z7fIeBWaWKg5qZpYqDmr9Y37hJFZl/DurUX6nZmap4pqamaWKg5qZpYqDWhlJmilppaRmSddVujxWmKQFkjZKeqbSZbHXxkGtTCTVA3OBC4ETgMsknVDZUlkRbgX6dbColZaDWvlMB5ojYlVEtAELgYsrXCYrICLuB7ZUuhz22jmolc94YE3OeUv2mpmVkYNa+aiXax4/Y1ZmDmrl0wJMzDmfAKyrUFnMBgwHtfJZChwrabKkQcAsYFGFy2SWeg5qZRIRHcAcYAnwLPDjiFhe2VJZIZJ+BDwETJHUIunKSpfJkvE0KTNLFdfUzCxVHNTMLFUc1MwsVRzUzCxVHNTMLFUc1GqIpE5JT0h6RtJPJA09gLxulXRp9vMt+SbbS5oh6fTX8Iw/Sdpv16G+rvdIsyPhs/6PpE8mLaOlj4NabXklIqZGxBuBNmB27s3syiCJRcRVEbEiT5IZQOKgZlYJDmq16wHgmGwt6teSbgOellQv6euSlkp6StI1AMq4SdIKSXcBh3VnJOk+SdOyn2dKelzSk5LulXQUmeD58Wwt8UxJYyT9NPuMpZLemv3uaEn3SPq9pO/S+/zXfUj6uaTHJC2XdHWPe9/IluVeSWOy114n6e7sdx6QdHxJ/mtaaniH9hokqYHMOm13Zy9NB94YES9kA8PWiDhF0mDgt5LuAU4CpgBvAg4HVgALeuQ7BvgecFY2r1ERsUXSPGBHRPxLNt1twDcj4kFJk8jMmng98HngwYi4QdI7gH2CVB8+lH3GQcBSST+NiM3AMODxiPiEpOuzec8hsyHK7Ih4XtKpwM3Aua/hP6OllINabTlI0hPZzw8A/0qmWfhoRLyQvX4+8Obu92XASOBY4CzgRxHRCayT9Kte8n8LcH93XhHR17pibwdOkPZUxA6WNCL7jL/IfvcuSS8V8TN9VNIl2c8Ts2XdDHQBt2ev/xC4Q9Lw7M/7k5xnDy7iGTaAOKjVllciYmruhewf987cS8BHImJJj3QXUXjpIxWRBjKvLU6LiFd6KUvR8+4kzSATIE+LiF2S7gOG9JE8ss99ued/A7NcfqeWPkuAayU1Akg6TtIw4H5gVvad2zjgnF6++xBwtqTJ2e+Oyl7fDozISXcPmaYg2XRTsx/vB96fvXYhcGiBso4EXsoGtOPJ1BS71QHdtc3LyTRrtwEvSPrL7DMk6cQCz7ABxkEtfW4h877s8ezmId8lUyP/GfA88DTwHeA3Pb8YEZvIvAe7Q9KT7G3+3Qlc0t1RAHwUmJbtiFjB3l7YfwLOkvQ4mWbw6gJlvRtokPQU8AXg4Zx7O4E3SHqMzDuzG7LX3w9cmS3fcrxEuvXgVTrMLFVcUzOzVHFQM7NUcVAzs1RxUDOzVHFQM7NUcVAzs1RxUDOzVPn/J6Bi567TtkMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(baseline_y_test, y_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Scores Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787197178305126"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.array(bert_X_test[['bert_score_0', 'bert_score_1']])\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(bert_y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert XGBoost Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932304074750163"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# train\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(bert_X_train, bert_y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(bert_X_test)\n",
    "\n",
    "accuracy = accuracy_score(bert_y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(bert_y_test, y_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train.to_csv('baseline_train.csv', index=False)\n",
    "baseline_test.to_csv('baseline_test.csv', index=False)\n",
    "bert_train.to_csv('bert_train.csv', index=False)\n",
    "bert_test.to_csv('bert_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['node_text'] == 'Morally significant free will does not just require choice, but also a meaningful range of choices. Freedom to choose whether to donate $99 or $100 to charity is not significant. A range of possible actions - including evil ones - would need to be present in order for free will to be significant.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
